# -*- coding: utf-8 -*-
"""Red Wine Quality [Andika Rahman Teja].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15kMZOi4ChE2Jvm-I78Bx7b69vBGheiyH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import randint
import os

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

df = pd.read_csv('/content/winequality-red.csv')
df.head()

"""# EDA (Exploratory Data Analysis)"""

df.info()

df.describe()

"""Dari sini, kita bisa menemukan beberapa fakta:
- Tidak ada data yang **NULL** (total data = 1599)
- Terdapat 12 fitur / kolom dan sesuai keterangan pemilik dataset bahwa fitur `quality` akan menjadi targetnya
- Semua data berbentuk numerik (*float* dan *int*)
- Fitur `quality` ternyata hanya bernilai dari 3 hingga 8 (tidak sesuai keterangan pemilik dataset bahwa bernilai 0 hingga 10)
- Fitur `pH` bernilai 2.74 hingga 4.01 sehingga menunjukkan bahwa dataset ini sesuai kondisi riil (*wine* bersifat asam)
"""

# Histogram
col_box = df.columns
color_palette = sns.color_palette("Set1", len(col_box))
plt.figure(figsize=(15, 10))

for i, column in enumerate(col_box, 1):
    plt.subplot((len(col_box) // 3) + 1, 3, i)
    sns.histplot(data=df, x=column, color=color_palette[i - 1], kde=True)
    plt.xlabel(column)
    plt.title(f'{column} Histogram')

plt.tight_layout()
plt.show()

# Boxplot
col_box = df.columns
color_palette = sns.color_palette("Set1", len(col_box))
plt.figure(figsize = (15, 10))

for i, column in enumerate(col_box, 1):
    plt.subplot((len(col_box)//3)+1, 3, i)
    sns.boxplot(data=df, x=column, color=color_palette[i - 1])
    plt.xlabel(column)
    plt.title(f'{column} Boxplot')

plt.tight_layout()
plt.show()

df.skew()

"""**Beberapa fitur terlihat skew kanan dan banyak outlier!!!**"""

# Hapus Outlier dengan Metode IQR
def drop_outliers(df, field_name):
    iqr = 1.5 * (np.percentile(df[field_name], 75) - np.percentile(df[field_name], 25))
    df.drop(df[df[field_name] > (iqr + np.percentile(df[field_name], 75))].index, inplace=True)
    df.drop(df[df[field_name] < (np.percentile(df[field_name], 25) - iqr)].index, inplace=True)

col_skew = ['chlorides', 'residual sugar', 'sulphates']
for col in col_skew:
  drop_outliers(df, col)

df.skew()

df.reset_index(inplace = True, drop = True)
df.info()

# kita cek lagi
col_box = df.columns
color_palette = sns.color_palette("Set1", len(col_box))
plt.figure(figsize = (15, 10))

for i, column in enumerate(col_box, 1):
    plt.subplot((len(col_box)//3)+1, 3, i)
    sns.boxplot(data=df, x=column, color=color_palette[i - 1])
    plt.xlabel(column)
    plt.title(f'{column} Boxplot')

plt.tight_layout()
plt.show()

"""**Berdasarkan `df.skew`, data sudah tidak terlalu skew dan tidak terlalu banyak outlier**
> Pertimbangkan juga jumlah datanya
"""

# Correlation Heatmap
plt.figure(figsize=(15,10))
sns.heatmap(df.corr(), cmap="YlGnBu", annot=True)

"""Berdasarkan korelasi tersebut, fitur `residual sugar` dan `free sulfur dioxide` akan di-*drop* karena memiliki korelasi yang kecil terhadap fitur target, yakni `quality`"""

df.drop(columns=['residual sugar', 'free sulfur dioxide'], inplace = True)

df.info()

quality_counts = df['quality'].value_counts().sort_index()

plt.bar(quality_counts.index, quality_counts.values)
plt.title('Jumlah Wine Berdasarkan Kualitasnya')
plt.xlabel('Kualitas')
plt.ylabel('Jumlah Wine')

plt.show()

"""Untuk dapat mempermudah klasifikasi, kita asumsikan:
- Kualitas Wine 3 hingga 5 => **Buruk** => 0
- Kualitas Wine 6 hingga 8 => **Bagus** => 1
"""

def categorize_quality(quality):
    if quality >= 6:
        return 1
    else:
        return 0

df['quality'] = df['quality'].apply(categorize_quality)

# Kita cek lagi
quality_counts = df['quality'].value_counts().sort_index()

plt.bar(quality_counts.index, quality_counts.values)
plt.title('Jumlah Wine Berdasarkan Kualitasnya')
plt.xlabel('Kualitas')
plt.ylabel('Jumlah Wine')

plt.show()

"""# Pemisahan Data Train dan Valid"""

X = df.drop(columns = ['quality'])
scaler = StandardScaler()
X_norm = scaler.fit_transform(X)
X = pd.DataFrame(X_norm, columns = X.columns)

y = df['quality']

X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state = 42, shuffle = True, test_size = 0.3)

"""# Prediksi dengan Model **AdaBoost**"""

AB = AdaBoostClassifier(random_state = 42)
AB.fit(X_train, y_train)
y_predict = AB.predict(X_valid)

print('Accuracy Score: ' + str(accuracy_score(y_valid, y_predict)))

# Hyperparameter Tuning
def randHyperParameterTuning(X_train, y_train):
    param_tuning = {
        'n_estimators': randint(50, 200),
        'learning_rate': [0.01, 0.05, 0.1, 0.3, 1]
    }

    AB = AdaBoostClassifier(random_state = 42)

    random_search = RandomizedSearchCV(estimator = AB,
                                 param_distributions = param_tuning,
                                 cv = 5,
                                 n_jobs = -1,
                                 verbose = 1)

    random_search.fit(X_train, y_train)
    return random_search.best_params_

rand_best_params = randHyperParameterTuning(X_train, y_train)

rand_best_params

AB = AdaBoostClassifier(**rand_best_params)
AB.fit(X_train, y_train)
y_predict = AB.predict(X_valid)

print('Accuracy Score: ' + str(accuracy_score(y_valid, y_predict)))

"""# Prediksi dengan Model **XGBoost**"""

xgb = XGBClassifier(random_state = 42)
xgb.fit(X_train, y_train)
y_predict = xgb.predict(X_valid)

print('Accuracy Score: ' + str(accuracy_score(y_valid, y_predict)))

def randHyperParameterTuning(X_train, y_train):
    param_tuning = {
        'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5],
        'max_depth': [3, 5, 6, 10],
        'min_child_weight': [1, 3, 5],
        'subsample': [0.5, 0.7, 1],
        'colsample_bytree': [0.5, 0.7, 1],
        'n_estimators' : [100, 200, 500],
    }

    xgb = XGBClassifier(random_state = 42)

    random_search = RandomizedSearchCV(estimator = xgb,
                                 param_distributions = param_tuning,
                                 cv = 5,
                                 n_jobs = -1,
                                 verbose = 1)

    random_search.fit(X_train, y_train)
    return random_search.best_params_

rand_best_params = randHyperParameterTuning(X_train, y_train)

rand_best_params

xgb = XGBClassifier(**rand_best_params)
xgb.fit(X_train, y_train)
y_predict = xgb.predict(X_valid)

print('Accuracy Score: ' + str(accuracy_score(y_valid, y_predict)))

"""# Prediksi dengan Model **ANN**"""

input_dim = len(X_train.columns)
ann = Sequential()

ann.add(Dense(128, activation = 'gelu', input_shape = (input_dim,)))
ann.add(Dropout(0.35))
ann.add(Dense(512, activation = 'gelu'))
ann.add(Dropout(0.35))
ann.add(Dense(512, activation = 'gelu'))
ann.add(Dropout(0.35))
ann.add(Dense(512, activation = 'gelu'))
ann.add(Dropout(0.35))
ann.add(Dense(1, activation = 'sigmoid'))

ann.summary()

checkpoint_path = 'training_model/cp.ckpt'
checkpoint_dir = os.path.dirname(checkpoint_path)

cp_callback = ModelCheckpoint(checkpoint_path,
                              save_weights_only=True,
                              verbose=1)

lr_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',
                                            patience=3,
                                            factor=0.5,
                                            min_lr = 1e-6,
                                            verbose = 1)

early_stoping = EarlyStopping(monitor='val_loss',patience= 5,restore_best_weights=True,verbose=1)

ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history = ann.fit (X_train, y_train,
                   validation_data = (X_valid, y_valid),
                   callbacks = [cp_callback, lr_reduction, early_stoping],
                   batch_size = 64,
                   epochs = 100)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

ann.evaluate(X_valid, y_valid)

"""**Arsitektur ANN ini adalah yang terbaik setelah beberapa percobaan mengganti ukuran layer dan fungsi aktivasinya**

# Kesimpulan
- Akurasi model AdaBoost = $\pm$ 73%
- Akurasi model XGBoost = $\pm$ 77%
- Akurasi model ANN = $\pm$ 72%

Oleh karena itu, pada kasus ini solusi terbaiknya adalah menggunakan model **XGBoost**
"""

xgb = XGBClassifier(**rand_best_params)
xgb.fit(X_train, y_train)
y_predict = xgb.predict(X_valid)

print('Accuracy Score: ' + str(accuracy_score(y_valid, y_predict)))
xgb_scores = cross_val_score(xgb, X_train, y_train, cv=5)
print("Cross-validated Accuracy: %0.2f (+/- %0.2f)" % (xgb_scores.mean(), xgb_scores.std() * 2))

print(classification_report(y_valid, y_predict))
cf_matrix = confusion_matrix(y_valid, y_predict)
sns.heatmap(cf_matrix, annot=True, fmt='')
plt.xlabel("Actual")
plt.ylabel('Prediction')
plt.title('Confusion Matrix')

"""# Sekian Terima Kasih

### Created by: Andika Rahman Teja
"""