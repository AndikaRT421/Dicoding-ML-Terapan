# -*- coding: utf-8 -*-
"""Dicoding_Recommendation_System_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_g3vHNmGikvN06Q9bYIr70gGdhLJKivU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping

"""# Dataset Anime"""

df_anime = pd.read_csv('/content/Anime_2023.csv')
df_anime.head(5)

"""## Data Preparation & Preprocessing"""

df_anime.info()

"""**Tidak ada nilai NULL**"""

df_anime['Score'].unique()

columns = ['anime_id', 'Name', 'Genres', 'Other name', 'Score', 'Type']

for i in columns:
  jumlah_unknown = df_anime[i].value_counts().get('UNKNOWN', 0)
  print(f"Jumlah {i} yang tidak diketahui => ", jumlah_unknown, "\n\n")

"""Karena dataset ini akan digunakan untuk sistem rekomendasi dengan teknik **Content Based Filtering**, sehingga fitur `Score` tidak terlalu berpengaruh karena sistem rekomendasi ini berdasarkan `Genres`. Namun karena jumlah UNKNOWN sebanyak 30% data, sehingga saya mengambil pilihan untuk tidak *drop* datanya melainkan menggantinya menjadi 0 dengan asumsi tidak ada *user* yang memberikan penilaian pada *anime* tersebut"""

df_anime['Score'] = df_anime['Score'].apply(lambda x: 0 if x == 'UNKNOWN' else x)

jumlah_unknown = df_anime['Score'].value_counts().get('UNKNOWN', 0)
print("Jumlah score yang tidak diketahui => ", jumlah_unknown)

df_anime['Score'] = df_anime['Score'].astype(float)
df_anime.describe().applymap(lambda x: f"{x:0.2f}")

# Histogram untuk fitur Score
col_box = ['Score']
color_palette = sns.color_palette("Set1", len(col_box))
plt.figure(figsize=(12, 10))

sns.histplot(data=df_anime, x='Score', color=color_palette[0], kde=True)
plt.xlabel('Score')
plt.title('Score Histogram')

plt.tight_layout()
plt.savefig('Score_Histogram.png')
plt.show()

"""Meskipun fitur `Other name` dan `Type` juga memiliki nilai UNKNOWN dengan persentase yang lebih sedikit dibanding fitur `Score`, saya tetap tidak melakukan *drop* data dikarenakan akan mempengaruhi sistem rekomendasi yang dibuat."""

# Barplot untuk fitur Type
plt.figure(figsize=(12, 10))
sns.countplot(data=df_anime, x='Type', hue='Type', palette='Set1')
plt.title('Type')

plt.savefig('Type_Barplot.png')
plt.tight_layout()
plt.show()

"""Untuk dapat memproses fitur `Genres`, saya ingin melakukan **One Hot Encoding** pada tiap genre yang ada. Namun karena hal tersebut akan menyebabkan ukuran file membesar dan terdapat keterbatasan komputasi sehingga saya akan hanya akan mengambil fitur `anime_id` dan `Name`"""

df_anime_ohe = df_anime[['anime_id', 'Name']]
genre_ohe = df_anime['Genres'].str.get_dummies(sep=', ')

df_anime_ohe = pd.concat([df_anime_ohe, genre_ohe], axis=1)

pd.set_option('display.max_columns', None)
df_anime_ohe.head(5)

df_anime_ohe.info()

"""## Cosine Similiarity dan Model Sistem Rekomendasi (Content Based Filtering)"""

cosine_sim = cosine_similarity(genre_ohe)
cs_df_anime = pd.DataFrame(cosine_sim, index=df_anime_ohe['Name'], columns=df_anime_ohe['Name'])
print('Shape:', cs_df_anime.shape)

pd.set_option('display.max_columns', 10)
cs_df_anime.head(10)

def anime_recommendations(judul_anime, similarity_data=cs_df_anime, items=df_anime[['Name', 'Genres']], k=5):
    index = similarity_data.loc[:,judul_anime].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(judul_anime, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Sebagai percobaan, saya akan mencari rekomendasi *anime* yang mirip dengan [Re:Zero kara Hajimeru Isekai Seikatsu](https://myanimelist.net/anime/31240/Re_Zero_kara_Hajimeru_Isekai_Seikatsu?q=re%3Azero%20&cat=anime) berdasarkan genrenya."""

# untuk memastikan bagaimana penulisan judulnya
cekk = df_anime[df_anime['Name'].str.contains('re:zero', case=False)]
cekk

anime_recommendations('Re:Zero kara Hajimeru Isekai Seikatsu', k = 10)

genre_cek = df_anime[df_anime['Genres'].str.contains('Drama, Fantasy, Suspense', case=False)]
genre_cek

"""Berdasarkan hasil rekomendasi di atas menunjukkan bahwa **akurasi model adalah 100%** dengan alasan:
- Seluruh *anime* dengan `Genres` berupa `Drama, Fantasy, Suspense` terdapat pada rekomendasi yang disarankan oleh model
- Karena `Genres` dengan tipe `Drama, Fantasy, Suspense` hanya terdapat 7 film (termasuk yang *anime* yang dicari), maka sisanya akan menyesuaikan ketiga genre tersebut.

# Dataset User Anime
"""

df_user = pd.read_csv('/content/User_anime_2023.csv')
df_user.head(5)

"""## Data Preparation & Preprocessing"""

df_user.info()

"""Terdapat nilai NULL pada `Username` dengan persentase yang sangat kecil (kurang dari 1%) sehingga saya akan *drop* datanya"""

df_user.dropna(inplace = True)
df_user.reset_index(inplace = True, drop = True)

df_user.info()

df_user.describe().applymap(lambda x: f"{x:0.2f}")

# Histogram user rating
col_box = ['rating']
color_palette = sns.color_palette("Set3", len(col_box))
plt.figure(figsize=(12, 10))

sns.histplot(data=df_user, x='rating', color=color_palette[0], bins = 10)
plt.xlabel('Rating')
plt.title('Rating Histogram')

plt.tight_layout()
plt.savefig('Rating_Histogram.png')
plt.show()

df_user.info()

df_anime_cut = df_anime[['anime_id', 'Genres']]
df_anime_cut

df_user = pd.merge(df_user, df_anime_cut, on='anime_id')
df_user

"""## Encoding + Build model"""

user_data = df_user['user_id'].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_data)}
userencoded2user = {i: x for i, x in enumerate(user_data)}

anime_data = df_user['anime_id'].unique().tolist()
anime2anime_encoded = {x: i for i, x in enumerate(anime_data)}
anime_encoded2anime = {i: x for i, x in enumerate(anime_data)}

df_user['user'] = df_user['user_id'].map(user2user_encoded)
df_user['anime'] = df_user['anime_id'].map(anime2anime_encoded)

X = df_user[['user', 'anime']]
y = df_user[['rating']]

scaler = StandardScaler()
y_norm = scaler.fit_transform(y)
y = pd.DataFrame(y_norm, columns = y.columns)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True)

class RecommenderNet(tf.keras.Model):

    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_anime = num_anime
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.anime_embedding = layers.Embedding(
            num_anime,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.anime_bias = layers.Embedding(num_anime, 1)
        self.flatten = layers.Flatten()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dense2 = layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:,0])
        user_bias = self.user_bias(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        anime_bias = self.anime_bias(inputs[:, 1])

        dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

        x = dot_user_anime + user_bias + anime_bias
        x = self.flatten(x)
        x = self.dense1(x)
        x = self.dense2(x)

        return x

# physical_devices = tf.config.list_physical_devices('GPU')
# if len(physical_devices) > 0:
#     device = "/GPU:0"
#     with tf.device(device):
#         model = RecommenderNet(df_user['user_id'].nunique(), df_user['anime_id'].nunique(), 32)
#     print("GPU is used.")
# else:
#     print("GPU is not available. Using CPU instead.")
#     model = RecommenderNet(df_user['user_id'].nunique(), df_user['anime_id'].nunique(), 32)

model = RecommenderNet(df_user['user_id'].nunique(), df_user['anime_id'].nunique(), 48) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

model.build((None, 2))
model.summary()

start_lr = 0.00001
min_lr = 0.00001
max_lr = 0.00005
batch_size = 10000

rampup_epochs = 3
sustain_epochs = 0
exp_decay = .8

def lrfn(epoch):
    if epoch < rampup_epochs:
        return (max_lr - start_lr) / rampup_epochs * epoch + start_lr
    elif epoch < rampup_epochs + sustain_epochs:
        return max_lr
    else:
        return (max_lr - min_lr) * exp_decay**(epoch - rampup_epochs - sustain_epochs) + min_lr

lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=0)

checkpoint_filepath = './checkpoint/myanimeweights.h5'

model_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,
                                    save_weights_only=True,
                                    monitor='val_loss',
                                    mode='min',
                                    save_best_only=True)

early_stopping = EarlyStopping(patience=3, monitor='val_loss', mode='min', restore_best_weights=True)

my_callbacks = [
    model_checkpoints,
    lr_callback,
    early_stopping
]

# Memulai training

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 32,
    epochs = 25,
    validation_data = (X_val, y_val),
    callbacks=my_callbacks
)

"""Meskipun nilai *loss* dan *val_loss* bernilai negatif, hal ini tidak menjadi masalah karena `BinaryCrossEntropy()` merupakan interpretasi dari **Negative Log Likelihood**. Hal ini terbukti dari RMSE dan val_RMSE yang masih berusaha mencari titik optimum pada model selama model melakukan proses *training*."""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Prediksi Rekomendasi Anime dengan Teknik Collaborative Filtering"""

user_id = df_user['user_id'].sample(5).iloc[0]
anime_watched = df_user[df_user['user_id'] == user_id]
username_selected = anime_watched[['Username']].iloc[0].values

anime_not_watched = df_user[~df_user['anime_id'].isin(anime_watched['anime_id'].values)]['anime_id']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime2anime_encoded.keys()))
)

anime_watched

anime_not_watched = [[anime2anime_encoded.get(x)] for x in anime_not_watched]

user_encoder = user2user_encoded.get(user_id)

user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

# user2user_encoded.get(53322)

# user_encoder

# len(user_anime_array)

ratings = model.predict(user_anime_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded2anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print("Rekomendasi untuk user: {}".format(user_id), "AKA {}".format(username_selected))
print("====" * 9)
print("Anime dengan rating tertinggi dari user")
print("----" * 8)
top_movies_user = (
    anime_watched.sort_values(by="rating", ascending=False)
    .head(5)
)
# anime_df_rows = df_user[df_user["anime_id"].isin(top_movies_user)]
# count = 0
for _, row in top_movies_user.iterrows():
    print(f"{row['Anime Title']} ==>\t{row['Genres']}")

print("----" * 8)
print("Top 10 rekomendasi anime")
print("----" * 8)
recommended_movies = df_user[df_user["anime_id"].isin(recommended_anime_ids)]

count = 0
displayed_titles = set()
for _, row in recommended_movies.iterrows():
    if row['Anime Title'] not in displayed_titles:
        print(row['Anime Title'], "\t ==>\t", row['Genres'])
        displayed_titles.add(row['Anime Title'])
        count += 1
    if count == 10:
        break

"""Dari *anime* yang pernah ditonton oleh *user* terpilih, model dapat memprediksi 10 *anime* yang cocok untuk *user*. Hasil rekomendasi yang disarankan memiliki **akurasi yang sangat baik**. Hal ini terbukti dari genre *anime* yang pernah ditonton oleh *user* sesuai dengan genre *anime* yang disarankan oleh model.

# Sekian Terima Kasih

### Created by: Andika Rahman Teja
"""